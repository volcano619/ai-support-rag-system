{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5745c642",
   "metadata": {},
   "source": [
    "# RAG System for IT Support (Local CPU Version)\n",
    "\n",
    "This notebook implements a complete Retrieve-Augmented Generation system using Flan-T5 and FAISS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f874270",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install Dependencies\n",
    "!pip install -q transformers sentence-transformers faiss-cpu pandas numpy torch accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b552120",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39da61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configuration Module for RAG System\n",
    "\n",
    "This module centralizes all configuration parameters for the RAG system.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# PROJECT PATHS\n",
    "# ============================================================================\n",
    "PROJECT_ROOT = Path(\".\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "FAISS_INDEX_DIR = DATA_DIR / \"faiss_index\"\n",
    "RESULTS_DIR = DATA_DIR / \"results\"\n",
    "\n",
    "for dir_path in [DATA_DIR, FAISS_INDEX_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_PATH = PROJECT_ROOT / \"rag_sample_qas_from_kis.csv\"\n",
    "\n",
    "# ============================================================================\n",
    "# EMBEDDING MODEL CONFIGURATION\n",
    "# ============================================================================\n",
    "# Small, efficient model (384 dims, ~80MB)\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ============================================================================\n",
    "# RETRIEVAL CONFIGURATION\n",
    "# ============================================================================\n",
    "RETRIEVAL_TOP_K = 5\n",
    "SIMILARITY_THRESHOLD = 0.3\n",
    "\n",
    "# ============================================================================\n",
    "# RERANKING CONFIGURATION\n",
    "# ============================================================================\n",
    "# Small cross-encoder (~80MB)\n",
    "# Small cross-encoder (~80MB)\n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "RERANK_TOP_K = 2  # Increased to 2 (with smaller chunks) to capture more context\n",
    "\n",
    "# ============================================================================\n",
    "# LLM CONFIGURATION (LOCAL FLAN-T5)\n",
    "# ============================================================================\n",
    "# Changed to local model for CPU compatibility\n",
    "USE_LOCAL_LLM = True\n",
    "LLM_MODEL_NAME = \"google/flan-t5-base\"  # ~250MB size, perfectly fits in 1.4GB RAM\n",
    "\n",
    "# Generation parameters\n",
    "LLM_TEMPERATURE = 0.0\n",
    "LLM_MAX_TOKENS = 512\n",
    "\n",
    "# ============================================================================\n",
    "# EVALUATION CONFIGURATION\n",
    "# ============================================================================\n",
    "RETRIEVAL_RECALL_TARGET = 0.95\n",
    "RESPONSE_SIMILARITY_TARGET = 0.90\n",
    "ROUGE_L_TARGET = 0.70\n",
    "\n",
    "# ============================================================================\n",
    "# APPLICATION CONFIGURATION\n",
    "# ============================================================================\n",
    "APP_TITLE = \"ðŸ¤– RAG-Based IT Support (Local)\"\n",
    "APP_LAYOUT = \"wide\"\n",
    "DEBUG_MODE = True\n",
    "\n",
    "# ============================================================================\n",
    "# DATA PROCESSING CONFIGURATION\n",
    "# ============================================================================\n",
    "COL_TOPIC = \"ki_topic\"\n",
    "COL_TEXT = \"ki_text\"\n",
    "COL_QUESTION = \"sample_question\"\n",
    "COL_GROUND_TRUTH = \"sample_ground_truth\"\n",
    "CHUNKING_ENABLED = False\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION\n",
    "# ============================================================================\n",
    "def validate_config():\n",
    "    if not DATASET_PATH.exists():\n",
    "        raise ValueError(f\"Dataset not found: {DATASET_PATH}\")\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0e8653",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Loader Module for RAG System\n",
    "\n",
    "This module handles loading and preprocessing the KIS Q&A dataset.\n",
    "Each function is documented with its purpose and reasoning.\n",
    "\n",
    "WHY THIS MODULE EXISTS:\n",
    "- Centralizes all data loading logic\n",
    "- Provides clean, validated data to other components\n",
    "- Handles edge cases (missing values, malformed data)\n",
    "- Creates structured objects that are easy to work with\n",
    "\n",
    "Author: RAG System\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "import logging\n",
    "\n",
    "\n",
    "    DATASET_PATH,\n",
    "    COL_TOPIC,\n",
    "    COL_TEXT,\n",
    "    COL_QUESTION,\n",
    "    COL_GROUND_TRUTH\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class KnowledgeItem:\n",
    "    \"\"\"\n",
    "    Represents a single knowledge base article\n",
    "    \n",
    "    WHY THIS CLASS:\n",
    "    - Provides a clean, typed interface for knowledge items\n",
    "    - Easier to work with than raw dictionaries\n",
    "    - Can add methods for processing/validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, topic: str, text: str, metadata: Dict = None):\n",
    "        self.topic = topic\n",
    "        self.text = text\n",
    "        self.metadata = metadata or {}\n",
    "        \n",
    "        # Generate a unique ID based on topic\n",
    "        # WHY: Needed for tracking which document was retrieved\n",
    "        self.id = self._generate_id()\n",
    "    \n",
    "    def _generate_id(self) -> str:\n",
    "        \"\"\"Generate unique ID from topic\"\"\"\n",
    "        # Simple slug generation: lowercase, replace spaces with underscores\n",
    "        return self.topic.lower().replace(\" \", \"_\")\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for serialization\"\"\"\n",
    "        return {\n",
    "            \"id\": self.id,\n",
    "            \"topic\": self.topic,\n",
    "            \"text\": self.text,\n",
    "            \"metadata\": self.metadata\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"KnowledgeItem(id='{self.id}', topic='{self.topic[:50]}...')\"\n",
    "\n",
    "\n",
    "class QAPair:\n",
    "    \"\"\"\n",
    "    Represents a question-answer pair for evaluation\n",
    "    \n",
    "    WHY THIS CLASS:\n",
    "    - Keeps test data organized\n",
    "    - Links questions to their expected answers\n",
    "    - Tracks which KI article should be retrieved\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, question: str, ground_truth: str, expected_ki_id: str):\n",
    "        self.question = question\n",
    "        self.ground_truth = ground_truth\n",
    "        self.expected_ki_id = expected_ki_id\n",
    "    \n",
    "    def to_dict(self) -> Dict:\n",
    "        \"\"\"Convert to dictionary for serialization\"\"\"\n",
    "        return {\n",
    "            \"question\": self.question,\n",
    "            \"ground_truth\": self.ground_truth,\n",
    "            \"expected_ki_id\": self.expected_ki_id\n",
    "        }\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"QAPair(question='{self.question[:50]}...', expected_ki='{self.expected_ki_id}')\"\n",
    "\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize text\n",
    "    \n",
    "    WHY MINIMAL CLEANING:\n",
    "    - The dataset is already high-quality corporate documentation\n",
    "    - Over-aggressive cleaning can remove important information\n",
    "    - Embeddings models handle varied text well\n",
    "    \n",
    "    Args:\n",
    "        text: Raw text string\n",
    "    \n",
    "    Returns:\n",
    "        Cleaned text string\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Remove excessive whitespace\n",
    "    # WHY: Multiple spaces/newlines don't add semantic value\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "    \n",
    "    # That's it! We want to preserve:\n",
    "    # - Punctuation (important for step-by-step instructions)\n",
    "    # - Numbers (version numbers, step numbers)\n",
    "    # - Special characters (e.g., file paths, commands)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def load_knowledge_base() -> List[KnowledgeItem]:\n",
    "    \"\"\"\n",
    "    Load all knowledge base articles from CSV with CHUNKING\n",
    "    \n",
    "    WHY CHUNKING:\n",
    "    - Small models (Flan-T5) have limited context (512 tokens)\n",
    "    - Original documents are ~1300 tokens, causing truncation\n",
    "    - We split docs into ~300 token chunks so the model sees the answer\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading knowledge base from {DATASET_PATH}\")\n",
    "    \n",
    "    # Check file exists\n",
    "    if not DATASET_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {DATASET_PATH}\")\n",
    "    \n",
    "    # Load CSV\n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    \n",
    "    knowledge_items = []\n",
    "    \n",
    "    # Configure chunking\n",
    "    # 600 chars ~= 150 tokens. Allows retrieving 2-3 chunks.\n",
    "    CHUNK_SIZE = 600  \n",
    "    CHUNK_OVERLAP = 100\n",
    "    \n",
    "    # Group by topic\n",
    "    unique_kis = df.groupby(COL_TOPIC).first().reset_index()\n",
    "    \n",
    "    for idx, row in unique_kis.iterrows():\n",
    "        topic = str(row[COL_TOPIC])\n",
    "        text = clean_text(str(row[COL_TEXT]))\n",
    "        \n",
    "        # Skip empty\n",
    "        if not topic or not text:\n",
    "            continue\n",
    "            \n",
    "        # Create chunks\n",
    "        chunks = []\n",
    "        if len(text) > CHUNK_SIZE:\n",
    "            start = 0\n",
    "            while start < len(text):\n",
    "                end = min(start + CHUNK_SIZE, len(text))\n",
    "                \n",
    "                # Attempt to split on space to avoid cutting words\n",
    "                if end < len(text):\n",
    "                    last_space = text.rfind(' ', start, end)\n",
    "                    if last_space != -1 and last_space > start + (CHUNK_SIZE // 2):\n",
    "                        end = last_space\n",
    "                \n",
    "                chunk_text = text[start:end]\n",
    "                chunks.append(chunk_text)\n",
    "                \n",
    "                # Move start forward by stride (size - overlap)\n",
    "                start += (CHUNK_SIZE - CHUNK_OVERLAP)\n",
    "        else:\n",
    "            chunks = [text]\n",
    "            \n",
    "        # Create KnowledgeItem for each chunk\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            # Create ID that links back to original topic but is unique for chunk\n",
    "            # e.g., \"email_setup_chunk_0\"\n",
    "            chunk_id = f\"{topic.lower().replace(' ', '_')}_{i}\"\n",
    "            \n",
    "            # Add context to text so model knows what this chunk is about\n",
    "            # WHY: Isolated chunks might lose context (e.g., \"Step 5: Click OK\")\n",
    "            # Adding title helps: \"Email Setup (Part 1): Step 5: Click OK\"\n",
    "            chunk_text_with_context = f\"{topic} (Part {i+1}):\\n{chunk}\"\n",
    "            \n",
    "            ki = KnowledgeItem(\n",
    "                topic=topic,\n",
    "                text=chunk_text_with_context, \n",
    "                metadata={\n",
    "                    \"source_row\": int(idx),\n",
    "                    \"chunk_index\": i,\n",
    "                    \"total_chunks\": len(chunks),\n",
    "                    \"is_chunk\": True,\n",
    "                    \"original_id\": topic.lower().replace(\" \", \"_\")\n",
    "                }\n",
    "            )\n",
    "            # Override ID manually to ensure uniqueness\n",
    "            ki.id = chunk_id\n",
    "            \n",
    "            knowledge_items.append(ki)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(knowledge_items)} chunks from {len(unique_kis)} original documents\")\n",
    "    return knowledge_items\n",
    "\n",
    "\n",
    "def load_qa_pairs() -> List[QAPair]:\n",
    "    \"\"\"\n",
    "    Load question-answer pairs for evaluation\n",
    "    \n",
    "    WHY THIS FUNCTION:\n",
    "    - Creates test set for accuracy measurement\n",
    "    - Links each question to its expected KI article\n",
    "    \n",
    "    Returns:\n",
    "        List of QAPair objects\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If dataset doesn't exist\n",
    "        ValueError: If dataset is malformed\n",
    "    \"\"\"\n",
    "    logger.info(f\"Loading Q&A pairs from {DATASET_PATH}\")\n",
    "    \n",
    "    if not DATASET_PATH.exists():\n",
    "        raise FileNotFoundError(f\"Dataset not found: {DATASET_PATH}\")\n",
    "    \n",
    "    df = pd.read_csv(DATASET_PATH)\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = [COL_TOPIC, COL_QUESTION, COL_GROUND_TRUTH]\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    qa_pairs = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        topic = row[COL_TOPIC]\n",
    "        question = row[COL_QUESTION]\n",
    "        ground_truth = row[COL_GROUND_TRUTH]\n",
    "        \n",
    "        # Clean text\n",
    "        clean_question = clean_text(str(question))\n",
    "        clean_ground_truth = clean_text(str(ground_truth))\n",
    "        \n",
    "        # Skip if any field is empty\n",
    "        if not clean_question or not clean_ground_truth:\n",
    "            logger.warning(f\"Skipping incomplete Q&A pair at row {idx}\")\n",
    "            continue\n",
    "        \n",
    "        # Generate expected KI ID from topic\n",
    "        # WHY: This links the question to which document should be retrieved\n",
    "        expected_ki_id = topic.lower().replace(\" \", \"_\")\n",
    "        \n",
    "        qa_pair = QAPair(\n",
    "            question=clean_question,\n",
    "            ground_truth=clean_ground_truth,\n",
    "            expected_ki_id=expected_ki_id\n",
    "        )\n",
    "        \n",
    "        qa_pairs.append(qa_pair)\n",
    "    \n",
    "    logger.info(f\"Loaded {len(qa_pairs)} Q&A pairs for evaluation\")\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "def load_all_data() -> Tuple[List[KnowledgeItem], List[QAPair]]:\n",
    "    \"\"\"\n",
    "    Load both knowledge base and Q&A pairs\n",
    "    \n",
    "    WHY THIS CONVENIENCE FUNCTION:\n",
    "    - Single function to load everything needed\n",
    "    - Ensures consistent loading\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (knowledge_items, qa_pairs)\n",
    "    \"\"\"\n",
    "    knowledge_items = load_knowledge_base()\n",
    "    qa_pairs = load_qa_pairs()\n",
    "    \n",
    "    logger.info(f\"Data loading complete: {len(knowledge_items)} KIs, {len(qa_pairs)} Q&A pairs\")\n",
    "    \n",
    "    return knowledge_items, qa_pairs\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Test the data loader\n",
    "    \n",
    "    WHY: Validates that data loading works correctly before using in pipeline\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING DATA LOADER\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        knowledge_items, qa_pairs = load_all_data()\n",
    "        \n",
    "        # Display sample knowledge item\n",
    "        print(\"\\nðŸ“š SAMPLE KNOWLEDGE ITEM:\")\n",
    "        print(\"-\" * 80)\n",
    "        sample_ki = knowledge_items[0]\n",
    "        print(f\"ID: {sample_ki.id}\")\n",
    "        print(f\"Topic: {sample_ki.topic}\")\n",
    "        print(f\"Text (first 200 chars): {sample_ki.text[:200]}...\")\n",
    "        print(f\"Metadata: {sample_ki.metadata}\")\n",
    "        \n",
    "        # Display sample Q&A pair\n",
    "        print(\"\\nâ“ SAMPLE Q&A PAIR:\")\n",
    "        print(\"-\" * 80)\n",
    "        sample_qa = qa_pairs[0]\n",
    "        print(f\"Question: {sample_qa.question}\")\n",
    "        print(f\"Expected KI: {sample_qa.expected_ki_id}\")\n",
    "        print(f\"Ground Truth (first 200 chars): {sample_qa.ground_truth[:200]}...\")\n",
    "        \n",
    "        # Statistics\n",
    "        print(\"\\nðŸ“Š DATASET STATISTICS:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Total Knowledge Items: {len(knowledge_items)}\")\n",
    "        print(f\"Total Q&A Pairs: {len(qa_pairs)}\")\n",
    "        print(f\"Average text length: {sum(ki.metadata['character_count'] for ki in knowledge_items) / len(knowledge_items):.0f} chars\")\n",
    "        print(f\"Average question length: {sum(len(qa.question) for qa in qa_pairs) / len(qa_pairs):.0f} chars\")\n",
    "        \n",
    "        # Verify all expected KI IDs exist in knowledge base\n",
    "        ki_ids = {ki.id for ki in knowledge_items}\n",
    "        missing_kis = [qa.expected_ki_id for qa in qa_pairs if qa.expected_ki_id not in ki_ids]\n",
    "        \n",
    "        if missing_kis:\n",
    "            print(f\"\\nâš ï¸  WARNING: {len(missing_kis)} Q&A pairs reference missing KIs:\")\n",
    "            for ki_id in set(missing_kis):\n",
    "                print(f\"  - {ki_id}\")\n",
    "        else:\n",
    "            print(\"\\nâœ… All Q&A pairs have corresponding knowledge items\")\n",
    "        \n",
    "        print(\"\\nâœ… DATA LOADER TEST PASSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ DATA LOADER TEST FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea602b91",
   "metadata": {},
   "source": [
    "## 3. Vector Database (FAISS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Vector Store Module for RAG System\n",
    "\n",
    "This module handles:\n",
    "1. Generating embeddings for knowledge base documents\n",
    "2. Building and managing the FAISS vector index\n",
    "3. Performing similarity search\n",
    "\n",
    "WHY THIS MODULE EXISTS:\n",
    "- Centralizes all vector database operations\n",
    "- Provides fast semantic search over knowledge base\n",
    "- Handles index persistence (save/load)\n",
    "\n",
    "WHY FAISS:\n",
    "- Extremely fast similarity search (optimized by Facebook AI)\n",
    "- Can scale to millions of documents\n",
    "- No external dependencies (runs locally)\n",
    "- Industry standard for vector search\n",
    "\n",
    "Author: RAG System\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import logging\n",
    "\n",
    "\n",
    "    EMBEDDING_MODEL_NAME,\n",
    "    FAISS_INDEX_DIR,\n",
    "    RETRIEVAL_TOP_K,\n",
    "    SIMILARITY_THRESHOLD\n",
    ")\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class VectorStore:\n",
    "    \"\"\"\n",
    "    Manages vector embeddings and FAISS index for semantic search\n",
    "    \n",
    "    WHY THIS CLASS:\n",
    "    - Encapsulates all vector database logic\n",
    "    - Provides clean API for retrieval\n",
    "    - Handles embedding generation and caching\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model_name: str = EMBEDDING_MODEL_NAME):\n",
    "        \"\"\"\n",
    "        Initialize vector store\n",
    "        \n",
    "        Args:\n",
    "            embedding_model_name: Name of the sentence-transformers model\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing VectorStore with model: {embedding_model_name}\")\n",
    "        \n",
    "        # Initialize embedding model\n",
    "        # WHY sentence-transformers:\n",
    "        # - Pre-trained on semantic similarity tasks\n",
    "        # - Produces high-quality embeddings for Q&A matching\n",
    "        # - Easy to use, well-maintained library\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "        \n",
    "        # Get embedding dimension\n",
    "        # WHY: Need this to initialize FAISS index with correct dimension\n",
    "        self.embedding_dim = self.embedding_model.get_sentence_embedding_dimension()\n",
    "        logger.info(f\"Embedding dimension: {self.embedding_dim}\")\n",
    "        \n",
    "        # Initialize FAISS index (will be built when add_documents is called)\n",
    "        self.index = None\n",
    "        \n",
    "        # Store knowledge items for retrieval\n",
    "        # WHY: After FAISS returns document IDs, we need to map back to original docs\n",
    "        self.knowledge_items: List[KnowledgeItem] = []\n",
    "        \n",
    "        # Store embeddings for potential reuse\n",
    "        self.embeddings: np.ndarray = None\n",
    "    \n",
    "    def _encode_texts(self, texts: List[str], show_progress: bool = True) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for texts\n",
    "        \n",
    "        WHY SEPARATE METHOD:\n",
    "        - Can be used for both documents and queries\n",
    "        - Centralizes encoding logic\n",
    "        - Easy to add batching, caching, etc.\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to encode\n",
    "            show_progress: Show progress bar\n",
    "        \n",
    "        Returns:\n",
    "            Numpy array of embeddings, shape (n_texts, embedding_dim)\n",
    "        \"\"\"\n",
    "        logger.info(f\"Encoding {len(texts)} texts...\")\n",
    "        \n",
    "        # Generate embeddings\n",
    "        # WHY normalize_embeddings=True:\n",
    "        # - Normalized vectors allow using cosine similarity via dot product\n",
    "        # - Faster computation and better numerical stability\n",
    "        # - FAISS IndexFlatIP (inner product) can be used for cosine similarity\n",
    "        embeddings = self.embedding_model.encode(\n",
    "            texts,\n",
    "            show_progress_bar=show_progress,\n",
    "            convert_to_numpy=True,\n",
    "            normalize_embeddings=True  # Critical for cosine similarity\n",
    "        )\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "    def build_index(self, knowledge_items: List[KnowledgeItem]):\n",
    "        \"\"\"\n",
    "        Build FAISS index from knowledge base documents\n",
    "        \n",
    "        WHY THIS METHOD:\n",
    "        - Creates the searchable vector database\n",
    "        - Must be called before search can be performed\n",
    "        \n",
    "        Args:\n",
    "            knowledge_items: List of KnowledgeItem objects to index\n",
    "        \"\"\"\n",
    "        logger.info(f\"Building FAISS index for {len(knowledge_items)} documents\")\n",
    "        \n",
    "        if not knowledge_items:\n",
    "            raise ValueError(\"No knowledge items provided for indexing\")\n",
    "        \n",
    "        # Store knowledge items\n",
    "        self.knowledge_items = knowledge_items\n",
    "        \n",
    "        # Extract texts for embedding\n",
    "        # WHY: We embed the full text of each KI article\n",
    "        # The topic is learned implicitly in the text content\n",
    "        texts = [ki.text for ki in knowledge_items]\n",
    "        \n",
    "        # Generate embeddings\n",
    "        self.embeddings = self._encode_texts(texts)\n",
    "        \n",
    "        logger.info(f\"Generated embeddings with shape: {self.embeddings.shape}\")\n",
    "        \n",
    "        # Build FAISS index\n",
    "        # WHY IndexFlatIP (Inner Product):\n",
    "        # - \"Flat\" = exhaustive search (guaranteed to find best matches)\n",
    "        # - \"IP\" = inner product (equivalent to cosine similarity for normalized vectors)\n",
    "        # - For small datasets (<10k docs), flat search is fast enough\n",
    "        # - For larger datasets, could use IndexIVFFlat for approximate search\n",
    "        self.index = faiss.IndexFlatIP(self.embedding_dim)\n",
    "        \n",
    "        # Add embeddings to index\n",
    "        # WHY: FAISS requires float32 numpy arrays\n",
    "        self.index.add(self.embeddings.astype('float32'))\n",
    "        \n",
    "        logger.info(f\"âœ… FAISS index built with {self.index.ntotal} vectors\")\n",
    "    \n",
    "    def search(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = RETRIEVAL_TOP_K,\n",
    "        score_threshold: float = SIMILARITY_THRESHOLD\n",
    "    ) -> List[Tuple[KnowledgeItem, float]]:\n",
    "        \"\"\"\n",
    "        Search for most similar documents to query\n",
    "        \n",
    "        WHY THIS METHOD:\n",
    "        - Core retrieval function for RAG system\n",
    "        - Returns ranked results with similarity scores\n",
    "        \n",
    "        Args:\n",
    "            query: User's question/query text\n",
    "            top_k: Number of results to return\n",
    "            score_threshold: Minimum similarity score (0-1)\n",
    "        \n",
    "        Returns:\n",
    "            List of (KnowledgeItem, similarity_score) tuples, sorted by score descending\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Index not built. Call build_index() first\")\n",
    "        \n",
    "        if not query or not query.strip():\n",
    "            logger.warning(\"Empty query provided\")\n",
    "            return []\n",
    "        \n",
    "        # Encode query\n",
    "        # WHY: Query needs to be in same embedding space as documents\n",
    "        query_embedding = self._encode_texts([query], show_progress=False)\n",
    "        \n",
    "        # Search FAISS index\n",
    "        # WHY top_k: We retrieve slightly more candidates for reranking\n",
    "        scores, indices = self.index.search(query_embedding.astype('float32'), top_k)\n",
    "        \n",
    "        # Process results\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            # Skip invalid indices (can happen if top_k > index size)\n",
    "            if idx < 0 or idx >= len(self.knowledge_items):\n",
    "                continue\n",
    "            \n",
    "            # Filter by score threshold\n",
    "            # WHY: Removes completely irrelevant results\n",
    "            if score < score_threshold:\n",
    "                logger.debug(f\"Skipping result with score {score:.3f} < threshold {score_threshold}\")\n",
    "                continue\n",
    "            \n",
    "            ki = self.knowledge_items[idx]\n",
    "            results.append((ki, float(score)))\n",
    "        \n",
    "        logger.info(f\"Found {len(results)} results for query (top_k={top_k}, threshold={score_threshold})\")\n",
    "        \n",
    "        if results:\n",
    "            logger.debug(f\"Top result: {results[0][0].topic} (score: {results[0][1]:.3f})\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save_index(self, index_dir: Path = FAISS_INDEX_DIR):\n",
    "        \"\"\"\n",
    "        Save FAISS index and metadata to disk\n",
    "        \n",
    "        WHY THIS METHOD:\n",
    "        - Avoids rebuilding index every time (expensive for large datasets)\n",
    "        - Enables loading pre-built index quickly\n",
    "        \n",
    "        Args:\n",
    "            index_dir: Directory to save index files\n",
    "        \"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"No index to save. Build index first\")\n",
    "        \n",
    "        index_dir = Path(index_dir)\n",
    "        index_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Save FAISS index\n",
    "        # WHY: FAISS provides optimized serialization\n",
    "        index_path = index_dir / \"faiss_index.bin\"\n",
    "        faiss.write_index(self.index, str(index_path))\n",
    "        logger.info(f\"Saved FAISS index to {index_path}\")\n",
    "        \n",
    "        # Save knowledge items\n",
    "        # WHY: Need to map FAISS indices back to original documents\n",
    "        ki_path = index_dir / \"knowledge_items.pkl\"\n",
    "        with open(ki_path, 'wb') as f:\n",
    "            pickle.dump(self.knowledge_items, f)\n",
    "        logger.info(f\"Saved knowledge items to {ki_path}\")\n",
    "        \n",
    "        # Save embeddings (optional, for analysis)\n",
    "        # WHY: Useful for debugging and analysis\n",
    "        emb_path = index_dir / \"embeddings.npy\"\n",
    "        np.save(emb_path, self.embeddings)\n",
    "        logger.info(f\"Saved embeddings to {emb_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            \"model_name\": EMBEDDING_MODEL_NAME,\n",
    "            \"embedding_dim\": self.embedding_dim,\n",
    "            \"num_documents\": len(self.knowledge_items)\n",
    "        }\n",
    "        metadata_path = index_dir / \"metadata.pkl\"\n",
    "        with open(metadata_path, 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        logger.info(f\"Saved metadata to {metadata_path}\")\n",
    "        \n",
    "        logger.info(\"âœ… Index saved successfully\")\n",
    "    \n",
    "    def load_index(self, index_dir: Path = FAISS_INDEX_DIR):\n",
    "        \"\"\"\n",
    "        Load FAISS index and metadata from disk\n",
    "        \n",
    "        WHY THIS METHOD:\n",
    "        - Fast loading of pre-built index\n",
    "        - Skips expensive embedding generation\n",
    "        \n",
    "        Args:\n",
    "            index_dir: Directory containing index files\n",
    "        \n",
    "        Raises:\n",
    "            FileNotFoundError: If index files don't exist\n",
    "        \"\"\"\n",
    "        index_dir = Path(index_dir)\n",
    "        \n",
    "        if not index_dir.exists():\n",
    "            raise FileNotFoundError(f\"Index directory not found: {index_dir}\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        index_path = index_dir / \"faiss_index.bin\"\n",
    "        if not index_path.exists():\n",
    "            raise FileNotFoundError(f\"FAISS index not found: {index_path}\")\n",
    "        \n",
    "        self.index = faiss.read_index(str(index_path))\n",
    "        logger.info(f\"Loaded FAISS index with {self.index.ntotal} vectors\")\n",
    "        \n",
    "        # Load knowledge items\n",
    "        ki_path = index_dir / \"knowledge_items.pkl\"\n",
    "        with open(ki_path, 'rb') as f:\n",
    "            self.knowledge_items = pickle.load(f)\n",
    "        logger.info(f\"Loaded {len(self.knowledge_items)} knowledge items\")\n",
    "        \n",
    "        # Load embeddings\n",
    "        emb_path = index_dir / \"embeddings.npy\"\n",
    "        if emb_path.exists():\n",
    "            self.embeddings = np.load(emb_path)\n",
    "            logger.info(f\"Loaded embeddings with shape: {self.embeddings.shape}\")\n",
    "        \n",
    "        logger.info(\"âœ… Index loaded successfully\")\n",
    "    \n",
    "    def index_exists(self, index_dir: Path = FAISS_INDEX_DIR) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a saved index exists\n",
    "        \n",
    "        WHY THIS METHOD:\n",
    "        - Allows conditional loading vs building\n",
    "        - Useful for caching logic\n",
    "        \n",
    "        Args:\n",
    "            index_dir: Directory to check\n",
    "        \n",
    "        Returns:\n",
    "            True if index files exist\n",
    "        \"\"\"\n",
    "        index_dir = Path(index_dir)\n",
    "        required_files = [\"faiss_index.bin\", \"knowledge_items.pkl\"]\n",
    "        return all((index_dir / f).exists() for f in required_files)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Test the vector store\n",
    "    \n",
    "    WHY: Validates that embedding and search work correctly\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING VECTOR STORE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Load knowledge base\n",
    "        print(\"\\nðŸ“š Loading knowledge base...\")\n",
    "        knowledge_items = load_knowledge_base()\n",
    "        \n",
    "        # Initialize vector store\n",
    "        print(\"\\nðŸ”§ Initializing vector store...\")\n",
    "        vector_store = VectorStore()\n",
    "        \n",
    "        # Build index\n",
    "        print(\"\\nðŸ—ï¸  Building FAISS index...\")\n",
    "        vector_store.build_index(knowledge_items)\n",
    "        \n",
    "        # Test search\n",
    "        print(\"\\nðŸ” Testing search...\")\n",
    "        test_queries = [\n",
    "            \"How do I reset my PIN?\",\n",
    "            \"Setting up email on my phone\",\n",
    "            \"VPN not connecting\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\nQuery: '{query}'\")\n",
    "            results = vector_store.search(query, top_k=3)\n",
    "            \n",
    "            if results:\n",
    "                for i, (ki, score) in enumerate(results, 1):\n",
    "                    print(f\"  {i}. {ki.topic} (score: {score:.3f})\")\n",
    "            else:\n",
    "                print(\"  No results found\")\n",
    "        \n",
    "        # Test save/load\n",
    "        print(\"\\nðŸ’¾ Testing save/load...\")\n",
    "        vector_store.save_index()\n",
    "        \n",
    "        # Create new instance and load\n",
    "        vector_store2 = VectorStore()\n",
    "        vector_store2.load_index()\n",
    "        \n",
    "        # Verify loaded index works\n",
    "        print(\"\\nðŸ” Testing loaded index...\")\n",
    "        results = vector_store2.search(\"How do I reset my PIN?\", top_k=3)\n",
    "        print(f\"Found {len(results)} results with loaded index\")\n",
    "        \n",
    "        print(\"\\nâœ… VECTOR STORE TEST PASSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ VECTOR STORE TEST FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a806c51",
   "metadata": {},
   "source": [
    "## 4. Retriever Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097244d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retriever Module for RAG System\n",
    "\n",
    "This module implements intelligent retrieval with two-stage ranking:\n",
    "1. Stage 1: Fast FAISS similarity search (retrieves top-K candidates)\n",
    "2. Stage 2: Cross-encoder reranking (reranks candidates for precision)\n",
    "\n",
    "WHY TWO-STAGE RETRIEVAL:\n",
    "- Stage 1 (Bi-encoder/FAISS): Fast but less accurate, high recall\n",
    "- Stage 2 (Cross-encoder): Slow but very accurate, high precision\n",
    "- Together: Best of both worlds - fast AND accurate\n",
    "\n",
    "WHY THIS IMPROVES ACCURACY:\n",
    "- Bi-encoders (FAISS) encode query and doc separately â†’ misses interaction\n",
    "- Cross-encoders see query+doc together â†’ captures semantic relationship\n",
    "- Studies show cross-encoder reranking improves accuracy by 10-15%\n",
    "\n",
    "Author: RAG System\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple, Dict\n",
    "from sentence_transformers import CrossEncoder\n",
    "import logging\n",
    "\n",
    "\n",
    "    RETRIEVAL_TOP_K,\n",
    "    RERANKER_MODEL_NAME,\n",
    "    RERANK_TOP_K,\n",
    "    SIMILARITY_THRESHOLD\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    \"\"\"\n",
    "    Intelligent retrieval system with two-stage ranking\n",
    "    \n",
    "    WHY THIS CLASS:\n",
    "    - Encapsulates complete retrieval pipeline\n",
    "    - Combines FAISS (speed) with cross-encoder (accuracy)\n",
    "    - Provides clean API for RAG system\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: VectorStore,\n",
    "        reranker_model_name: str = RERANKER_MODEL_NAME\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Initialized VectorStore instance\n",
    "            reranker_model_name: Name of cross-encoder model for reranking\n",
    "        \"\"\"\n",
    "        logger.info(\"Initializing Retriever with reranking\")\n",
    "        \n",
    "        self.vector_store = vector_store\n",
    "        \n",
    "        # Initialize cross-encoder for reranking\n",
    "        # WHY cross-encoder:\n",
    "        # - Processes (query, document) pair together\n",
    "        # - Learns interaction between query and doc\n",
    "        # - More accurate than bi-encoder for final ranking\n",
    "        # - Slower than bi-encoder, but we only rerank top-K (e.g., 5 docs)\n",
    "        logger.info(f\"Loading cross-encoder: {reranker_model_name}\")\n",
    "        self.reranker = CrossEncoder(reranker_model_name)\n",
    "        \n",
    "        logger.info(\"âœ… Retriever initialized\")\n",
    "    \n",
    "    def retrieve(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = RERANK_TOP_K,\n",
    "        retrieval_k: int = RETRIEVAL_TOP_K,\n",
    "        use_reranking: bool = True\n",
    "    ) -> List[Tuple[KnowledgeItem, float, Dict]]:\n",
    "        \"\"\"\n",
    "        Retrieve most relevant documents for query\n",
    "        \n",
    "        PIPELINE:\n",
    "        1. Stage 1: FAISS retrieves top-retrieval_k candidates (fast, high recall)\n",
    "        2. Stage 2: Cross-encoder reranks to top-k (slow, high precision)\n",
    "        \n",
    "        WHY THIS APPROACH:\n",
    "        - FAISS searches entire knowledge base quickly\n",
    "        - Cross-encoder only processes top candidates (5-10 docs)\n",
    "        - Result: Fast retrieval with high accuracy\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            top_k: Number of final results to return\n",
    "            retrieval_k: Number of candidates for stage 1 (should be >= top_k)\n",
    "            use_reranking: Whether to use cross-encoder reranking\n",
    "        \n",
    "        Returns:\n",
    "            List of (KnowledgeItem, score, metadata) tuples, sorted by score descending\n",
    "            metadata contains: stage1_score, stage2_score (if reranking), rank, etc.\n",
    "        \"\"\"\n",
    "        if not query or not query.strip():\n",
    "            logger.warning(\"Empty query provided\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Retrieving documents for query: '{query[:100]}...'\")\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 1: Fast FAISS Similarity Search\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.debug(f\"Stage 1: FAISS search (top_k={retrieval_k})\")\n",
    "        \n",
    "        # Get candidates from vector store\n",
    "        # WHY retrieval_k > top_k:\n",
    "        # - Casts wider net to ensure relevant docs are captured\n",
    "        # - Reranking will filter down to top_k most relevant\n",
    "        stage1_results = self.vector_store.search(\n",
    "            query=query,\n",
    "            top_k=retrieval_k,\n",
    "            score_threshold=SIMILARITY_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        if not stage1_results:\n",
    "            logger.warning(\"No results from stage 1 (FAISS search)\")\n",
    "            return []\n",
    "        \n",
    "        logger.info(f\"Stage 1: Found {len(stage1_results)} candidates\")\n",
    "        \n",
    "        # If reranking disabled, return stage 1 results\n",
    "        if not use_reranking:\n",
    "            logger.info(\"Reranking disabled, returning stage 1 results\")\n",
    "            results = []\n",
    "            for i, (ki, score) in enumerate(stage1_results[:top_k], 1):\n",
    "                metadata = {\n",
    "                    \"stage1_score\": score,\n",
    "                    \"stage1_rank\": i,\n",
    "                    \"reranked\": False\n",
    "                }\n",
    "                results.append((ki, score, metadata))\n",
    "            return results\n",
    "        \n",
    "        # =====================================================================\n",
    "        # STAGE 2: Cross-Encoder Reranking\n",
    "        # =====================================================================\n",
    "        \n",
    "        logger.debug(f\"Stage 2: Cross-encoder reranking (top_k={top_k})\")\n",
    "        \n",
    "        # Prepare (query, document) pairs for cross-encoder\n",
    "        # WHY: Cross-encoder requires both query and doc as input\n",
    "        pairs = [(query, ki.text) for ki, _ in stage1_results]\n",
    "        \n",
    "        # Get reranking scores\n",
    "        # WHY: Cross-encoder outputs a single score for each (query, doc) pair\n",
    "        # Higher score = more relevant\n",
    "        rerank_scores = self.reranker.predict(pairs)\n",
    "        \n",
    "        # Combine results with both stage 1 and stage 2 scores\n",
    "        combined_results = []\n",
    "        for (ki, stage1_score), rerank_score in zip(stage1_results, rerank_scores):\n",
    "            combined_results.append({\n",
    "                \"ki\": ki,\n",
    "                \"stage1_score\": float(stage1_score),\n",
    "                \"stage2_score\": float(rerank_score),\n",
    "                \"final_score\": float(rerank_score)  # Use stage 2 score as final\n",
    "            })\n",
    "        \n",
    "        # Sort by reranking score (descending)\n",
    "        # WHY: Cross-encoder scores are more accurate for final ranking\n",
    "        combined_results.sort(key=lambda x: x[\"final_score\"], reverse=True)\n",
    "        \n",
    "        # Take top-k\n",
    "        top_results = combined_results[:top_k]\n",
    "        \n",
    "        # Format output\n",
    "        results = []\n",
    "        for i, result in enumerate(top_results, 1):\n",
    "            metadata = {\n",
    "                \"stage1_score\": result[\"stage1_score\"],\n",
    "                \"stage2_score\": result[\"stage2_score\"],\n",
    "                \"final_score\": result[\"final_score\"],\n",
    "                \"stage1_rank\": stage1_results.index((result[\"ki\"], result[\"stage1_score\"])) + 1,\n",
    "                \"stage2_rank\": i,\n",
    "                \"reranked\": True\n",
    "            }\n",
    "            results.append((result[\"ki\"], result[\"final_score\"], metadata))\n",
    "        \n",
    "        logger.info(f\"Stage 2: Reranked to top {len(results)} results\")\n",
    "        \n",
    "        if results:\n",
    "            top_result = results[0]\n",
    "            logger.info(\n",
    "                f\"Top result: '{top_result[0].topic}' \"\n",
    "                f\"(stage1_score: {top_result[2]['stage1_score']:.3f}, \"\n",
    "                f\"stage2_score: {top_result[2]['stage2_score']:.3f})\"\n",
    "            )\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_context_for_generation(\n",
    "        self,\n",
    "        query: str,\n",
    "        top_k: int = RERANK_TOP_K\n",
    "    ) -> Tuple[str, List[Dict]]:\n",
    "        \"\"\"\n",
    "        Get formatted context for LLM generation\n",
    "        \n",
    "        WHY THIS METHOD:\n",
    "        - Retrieves relevant docs and formats them for LLM\n",
    "        - Returns both context string and metadata\n",
    "        - Makes it easy to feed into generator\n",
    "        \n",
    "        Args:\n",
    "            query: User's question\n",
    "            top_k: Number of documents to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (context_string, sources_metadata)\n",
    "            - context_string: Formatted text to inject into LLM prompt\n",
    "            - sources_metadata: List of dicts with source information\n",
    "        \"\"\"\n",
    "        # Retrieve documents\n",
    "        results = self.retrieve(query, top_k=top_k)\n",
    "        \n",
    "        if not results:\n",
    "            logger.warning(\"No documents retrieved for context\")\n",
    "            return \"\", []\n",
    "        \n",
    "        # Format context\n",
    "        # WHY: Clear structure helps LLM understand and cite sources\n",
    "        context_parts = []\n",
    "        sources_metadata = []\n",
    "        \n",
    "        for i, (ki, score, metadata) in enumerate(results, 1):\n",
    "            # Add document to context\n",
    "            # WHY numbered sections: Makes it easy for LLM to cite sources\n",
    "            context_parts.append(f\"[Document {i}: {ki.topic}]\\n{ki.text}\\n\")\n",
    "            \n",
    "            # Track source metadata\n",
    "            sources_metadata.append({\n",
    "                \"number\": i,\n",
    "                \"id\": ki.id,\n",
    "                \"topic\": ki.topic,\n",
    "                \"score\": score,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "        \n",
    "        context_string = \"\\n---\\n\\n\".join(context_parts)\n",
    "        \n",
    "        logger.info(f\"Generated context from {len(results)} documents ({len(context_string)} characters)\")\n",
    "        \n",
    "        return context_string, sources_metadata\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TESTING & VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Test the retriever with reranking\n",
    "    \n",
    "    WHY: Validates that two-stage retrieval improves accuracy\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING RETRIEVER WITH RERANKING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    \n",
    "    \n",
    "    try:\n",
    "        # Load knowledge base\n",
    "        print(\"\\nðŸ“š Loading knowledge base...\")\n",
    "        knowledge_items = load_knowledge_base()\n",
    "        \n",
    "        # Initialize vector store\n",
    "        print(\"\\nðŸ”§ Initializing vector store...\")\n",
    "        vector_store = VectorStore()\n",
    "        \n",
    "        # Check if index exists, otherwise build\n",
    "        if vector_store.index_exists():\n",
    "            print(\"Loading existing index...\")\n",
    "            vector_store.load_index()\n",
    "        else:\n",
    "            print(\"Building new index...\")\n",
    "            vector_store.build_index(knowledge_items)\n",
    "            vector_store.save_index()\n",
    "        \n",
    "        # Initialize retriever\n",
    "        print(\"\\nðŸŽ¯ Initializing retriever...\")\n",
    "        retriever = Retriever(vector_store)\n",
    "        \n",
    "        # Test queries\n",
    "        test_queries = [\n",
    "            \"How do I reset my forgotten PIN?\",\n",
    "            \"I need to set up company email on my Android phone\",\n",
    "            \"My printer is jammed, what should I do?\",\n",
    "            \"How can I configure VPN to work from home?\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(\"\\n\" + \"=\"*80)\n",
    "            print(f\"ðŸ“ Query: '{query}'\")\n",
    "            print(\"=\"*80)\n",
    "            \n",
    "            # Test WITHOUT reranking\n",
    "            print(\"\\nðŸ” Stage 1 Only (FAISS):\")\n",
    "            results_no_rerank = retriever.retrieve(query, top_k=3, use_reranking=False)\n",
    "            for i, (ki, score, metadata) in enumerate(results_no_rerank, 1):\n",
    "                print(f\"  {i}. {ki.topic}\")\n",
    "                print(f\"     Score: {score:.3f}\")\n",
    "            \n",
    "            # Test WITH reranking\n",
    "            print(\"\\nðŸŽ¯ Stage 1 + Stage 2 (FAISS + Reranking):\")\n",
    "            results_rerank = retriever.retrieve(query, top_k=3, use_reranking=True)\n",
    "            for i, (ki, score, metadata) in enumerate(results_rerank, 1):\n",
    "                print(f\"  {i}. {ki.topic}\")\n",
    "                print(f\"     Stage 1 Score: {metadata['stage1_score']:.3f}\")\n",
    "                print(f\"     Stage 2 Score: {metadata['stage2_score']:.3f}\")\n",
    "                print(f\"     Rank Change: {metadata['stage1_rank']} â†’ {metadata['stage2_rank']}\")\n",
    "            \n",
    "            # Get formatted context\n",
    "            print(\"\\nðŸ“„ Formatted Context:\")\n",
    "            context, sources = retriever.get_context_for_generation(query, top_k=2)\n",
    "            print(f\"  Context length: {len(context)} characters\")\n",
    "            print(f\"  Sources: {[s['topic'] for s in sources]}\")\n",
    "        \n",
    "        print(\"\\nâœ… RETRIEVER TEST PASSED\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ RETRIEVER TEST FAILED: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e357fec8",
   "metadata": {},
   "source": [
    "## 5. Generator (Flan-T5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad004a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generator Module for RAG System\n",
    "\n",
    "This module handles Response generation using a LOCAL LLM (Flan-T5).\n",
    "\n",
    "WHY LOCAL LLM:\n",
    "- User requirement: \"No API Key\" architecture\n",
    "- Hardware constraint: Low RAM (<2GB)\n",
    "- \"google/flan-t5-base\" is chosen for efficiency and robustness\n",
    "\n",
    "Author: RAG System\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Generator:\n",
    "    \"\"\"\n",
    "    Local LLM response generator using HuggingFace Transformers\n",
    "    \n",
    "    WHY THIS IMPLEMENTATION:\n",
    "    - Runs entirely offline\n",
    "    - Uses Seq2Seq model (T5) which is great for \"text-to-text\" tasks like QA\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = LLM_MODEL_NAME):\n",
    "        \"\"\"\n",
    "        Initialize local generator\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name (e.g., \"google/flan-t5-base\")\n",
    "        \"\"\"\n",
    "        logger.info(f\"Initializing Local Generator with model: {model_name}\")\n",
    "        \n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            \n",
    "            # Load model\n",
    "            # WHY device_map=\"cpu\":\n",
    "            # - We know system is RAM constrained and lacks NVIDIA GPU\n",
    "            # - \"auto\" might try to use GPU and fail\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                model_name,\n",
    "                device_map=\"cpu\", \n",
    "                torch_dtype=torch.float32 # FP32 is safer for CPU\n",
    "            )\n",
    "            \n",
    "            logger.info(\"âœ… Local Generator initialized successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load local model: {e}\")\n",
    "            raise RuntimeError(f\"Could not load model {model_name}. Check internet connection or RAM.\") from e\n",
    "\n",
    "    def _create_prompt(self, query: str, context: str) -> str:\n",
    "        \"\"\"\n",
    "        Create prompt formatted for Flan-T5 (Question First Strategy)\n",
    "        \n",
    "        WHY QUESTION FIRST:\n",
    "        - T5 has a 512 token limit.\n",
    "        - If we put Context first, the Question at the end gets truncated.\n",
    "        - By putting Question first, the model always knows WHAT to do, even if context is cut.\n",
    "        \"\"\"\n",
    "        # T5 prefers: \"question: ... context: ...\"\n",
    "        # Using standard T5 prefix format\n",
    "        prompt = (\n",
    "            f\"question: {query} \"\n",
    "            f\"context: {context}\"\n",
    "        )\n",
    "        return prompt\n",
    "    \n",
    "    def generate_response(\n",
    "        self,\n",
    "        query: str,\n",
    "        context: str,\n",
    "        sources_metadata: List[Dict],\n",
    "        temperature: float = LLM_TEMPERATURE,\n",
    "        max_tokens: int = LLM_MAX_TOKENS\n",
    "    ) -> Tuple[str, Dict]:\n",
    "        \"\"\"\n",
    "        Generate response using local LLM\n",
    "        \"\"\"\n",
    "        if not context:\n",
    "            return \"I don't have enough information to answer that question.\", {\"error\": \"no_context\"}\n",
    "\n",
    "        logger.info(f\"Generating response for query: '{query[:50]}...'\")\n",
    "        \n",
    "        # Prepare prompt\n",
    "        prompt = self._create_prompt(query, context)\n",
    "        \n",
    "        try:\n",
    "            # Tokenize\n",
    "            inputs = self.tokenizer(\n",
    "                prompt, \n",
    "                return_tensors=\"pt\", \n",
    "                max_length=512, \n",
    "                truncation=True\n",
    "            )\n",
    "            \n",
    "            # Generate\n",
    "            # WHY generation parameters:\n",
    "            # - do_sample=False: Deterministic greedy decoding (temperature ignored)\n",
    "            # - max_length: Limit response size\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_tokens,\n",
    "                do_sample=(temperature > 0),\n",
    "                temperature=temperature if temperature > 0 else None,\n",
    "            )\n",
    "            \n",
    "            # Decode\n",
    "            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Metadata\n",
    "            metadata = {\n",
    "                \"model\": LLM_MODEL_NAME,\n",
    "                \"sources_used\": [s['topic'] for s in sources_metadata]\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Generated response: {response_text[:50]}...\")\n",
    "            return response_text, metadata\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation failed: {e}\")\n",
    "            return \"Error generating response.\", {\"error\": str(e)}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing Local Generator...\")\n",
    "    gen = Generator()\n",
    "    ctx = \"To reset your password, visit password.corp.com and enter your employee ID.\"\n",
    "    q = \"Where do I go to reset my password?\"\n",
    "    res, _ = gen.generate_response(q, ctx, [])\n",
    "    print(f\"Query: {q}\\nResponse: {res}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483180a2",
   "metadata": {},
   "source": [
    "## 6. Execution & Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8320e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "# Setup Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Initialize System\n",
    "print(\"Initializing System...\")\n",
    "\n",
    "# 1. Load Data\n",
    "try:\n",
    "    knowledge_items = load_knowledge_base()\n",
    "except FileNotFoundError:\n",
    "    print(\"Please upload 'rag_sample_qas_from_kis.csv' to the current directory.\")\n",
    "    knowledge_items = []\n",
    "\n",
    "if knowledge_items:\n",
    "    # 2. Build Index\n",
    "    vector_store = VectorStore()\n",
    "    vector_store.build_index(knowledge_items)\n",
    "\n",
    "    # 3. Init Components\n",
    "    retriever = Retriever(vector_store)\n",
    "    generator = Generator()\n",
    "\n",
    "    print(\"\\nâœ… System Initialized!\\n\")\n",
    "\n",
    "    # Interactive Loop\n",
    "    def ask(query):\n",
    "        print(f\"â“ Question: {query}\")\n",
    "        print(\"ðŸ” Retrieving...\")\n",
    "        context, sources = retriever.get_context_for_generation(query, top_k=2)\n",
    "        \n",
    "        for s in sources:\n",
    "            print(f\"  - Found: {s['topic']} (Score: {s['score']:.3f})\")\n",
    "\n",
    "        print(\"ðŸ§  Generating...\")\n",
    "        response, _ = generator.generate_response(query, context, sources)\n",
    "        print(f\"ðŸ¤– Answer: {response}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    # Examples\n",
    "    ask(\"How do I reset my PIN?\")\n",
    "    ask(\"How do I configure VPN?\")\n",
    "else:\n",
    "    print(\"Using dummy data or failed to load.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
